{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report for Udacity Deep Reinforement Learning\n",
    "# Project 3: Collaboration and Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This report is for the Collaboration and Competition project at Deep Reinforcement Learning course by Udacity. The objective of the project is to train two agents control rackets to bounce a ball over a net as many times as possible. \n",
    "If an agent hits the ball over the net, it receives a reward of +0.1. If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Algorithm\n",
    "\n",
    "The same learning algorithm with [p2-ball-tracking](https://github.com/yshk-mrt/p2-ball-tracking) is applied.\n",
    "\n",
    "[A2C](https://openai.com/blog/baselines-acktr-a2c/) is used, which is a synchronous variant of [A3C](https://arxiv.org/abs/1602.01783). Pseudocode of A3C from the cited paper is as follows.\n",
    "\n",
    "<img src=\"files/content/a3c_suedocode.png\" width=\"800\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "One neural network for policy $\\pi(a|s;\\theta')$ and value function $V(s;\\theta'_v)$ is shared by all agents. The first and second hidden fully connected layers are shared by policy (actor) and value function (critic). In order to generate actions for continuous control, the network generates mean and standard deviation of Gaussian distribution, and actions are sampled from the distribution. Here is the corresponding code in [TrainAgent.ipynb](./TrainAgent.ipynb)\n",
    "\n",
    "\n",
    "```python\n",
    "    out1 = F.relu(self.fc1(state))\n",
    "    out2 = F.relu(self.fc2(out1))\n",
    "\n",
    "    # mean of the Gaussian distribution range in [-1, 1]\n",
    "    mean = torch.tanh(self.fc_actor(out2))\n",
    "    # V value\n",
    "    value = self.fc_critic(out2)\n",
    "\n",
    "    # Create distribution from mean and standard deviation\n",
    "    # Use softplus function to make deviation always positive\n",
    "    # SoftPlus is a smooth approximation to ReLU function\n",
    "    dist = torch.distributions.Normal(mean, F.softplus(self.std))\n",
    "\n",
    "    # Sample next action from the distribution.\n",
    "    action = dist.sample()\n",
    "    action = torch.clamp(action, min=-1.0, max=1.0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model structure\n",
    "\n",
    "Model structure by torchviz library.\n",
    "<img src=\"files/content/model.png\" width=\"800\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model parameters\n",
    "The following table shows model parameters. Linear-1 and Linear-2 is hidden layers, Linear-3 is for actions, and Linear-4 is for value function.\n",
    "```\n",
    "----------------------------------------------------------------\n",
    "        Layer (type)               Output Shape         Param #\n",
    "================================================================\n",
    "            Linear-1               [-1, 1, 128]           3,200\n",
    "            Linear-2                [-1, 1, 64]           8,256\n",
    "            Linear-3                 [-1, 1, 2]             130\n",
    "            Linear-4                 [-1, 1, 1]              65\n",
    "================================================================\n",
    "Total params: 11,651\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Hyperparameters\n",
    "The following hyperparameters are used.\n",
    "\n",
    "| Parameter                       | Value         | Description                                                    |\n",
    "|:--------------------------------|--------------:|:--------------------------------------------------------------|\n",
    "| N-Step                          |             5 | Update model every n-step                                     |\n",
    "| Discount factor \\[$ \\gamma $\\]  |          0.99 | Discount factor used in Q-Learning                            |\n",
    "| Critic_loss_coef                |         100.0 | Weight of critic loss                                         |\n",
    "| Entropy_loss_coef               |        0.0001 | Weight of entropy loss                                        |\n",
    "| Learning rate \\[$ \\alpha $\\]    |        0.0005 | Learning rate for Adam                                        |\n",
    "| Standard deviation              |           0.0 | Initial standard deviation before SoftPlus is applied         |\n",
    "| Max norm of the gradients       |           5.0 | Max norm for gradient clipping                                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### Plot of Rewards\n",
    "At episode 6921, A2C achieved score 0.5.  \n",
    "At episode 19779, A2C achieved average score 0.5.  \n",
    "Thus, the environment is considered solved.\n",
    "![trained_result](files/content/score.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained agents' behavior\n",
    "\n",
    "The following gif shows trained agent behaviors. This time, standard deviation for action generation is set to nearly zero, which means agents always take greedy actions. As the result, agents got an average score 1.91 over 100 episodes.\n",
    "![trained_agent](files/results/final/tennis.gif)\n",
    "![trained_agent_score](files/content/DetermisticScore.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consideration\n",
    "\n",
    "1. Entropy\n",
    "\n",
    "The following graph shows the average score and entropy. When the score improves, the entropy decreases. On the other hand, when the score drops, the entropy increases, which will contribute to recover the score again by searching broader action spaces. The weight of entropy is important to balance convergent and divergent mode.\n",
    "\n",
    "![entropy](files/content/score_entropy.png)\n",
    "\n",
    "2. Weight of critic loss\n",
    "\n",
    "The following graph (a) shows actor and critic loss without weight of critic loss. Graph (b) shows absolute value of actor loss divided by critic loss. As you can see, the actor loss is 10 to 1000 times bigger than the critic loss, and the ratio decreases as the learning continues. If the weight can be adaptively changed to keep the ratio, learning could be more stable. (A quick experiment did not work.)\n",
    "In this report, 100.0 is used. \n",
    "\n",
    "| ![actor_critic]| ![actor_divided_by_critic]|\n",
    "|---|---|\n",
    "|(a)Actor loss and critic loss|(b)Actor loss divided by critic loss|\n",
    "\n",
    "[actor_critic]:files/content/actor_critic.png\n",
    "[actor_divided_by_critic]:files/content/actor_critic_log.png\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for Future Work\n",
    "\n",
    "For more stable results:\n",
    " * Add a small value to std to prevent log probability becomes infinity when std is zero.\n",
    " * Use gradient clipping -> Done\n",
    " * Use Trust Region Policy Optimization (TRPO)\n",
    " * Try RMSprop instead of Adam\n",
    " * Try off-policy methods\n",
    " * Try separated network for actor and critic\n",
    " * Try adaptive critic loss weight mention in the second consideration point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* [GitHub:ShangtongZhang/DeepRL](https://github.com/ShangtongZhang/DeepRL)\n",
    "* [GitHub:qiaochen/A2C](https://github.com/qiaochen/A2C)\n",
    "* [Let’s make an A3C: Implementation](https://jaromiru.com/2017/03/26/lets-make-an-a3c-implementation)\n",
    "* [Understanding Actor Critic Methods and A2C](https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f)\n",
    "* [ゼロから作る A3C](https://qiita.com/s05319ss/items/2fe9bfe562fea1707e79)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "Code for graph of this report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def moving_average(data, points=100):\n",
    "    b=np.ones(points)/points\n",
    "    a = np.zeros(points-1)\n",
    "    return np.convolve(np.hstack((a, data)), b, mode='vaild')\n",
    "    \n",
    "def plot(data_source):\n",
    "    \"\"\"Plot data\n",
    "        Parameters\n",
    "        ----------\n",
    "            data_source : list\n",
    "                [Data label, data file path, line type]  \n",
    "    \"\"\"\n",
    "    plt.rcParams[\"figure.dpi\"] = 100.0\n",
    "    max_length = 30000\n",
    "    \n",
    "    for source in data_source:\n",
    "        data = np.loadtxt(source[1], skiprows=1, delimiter=', \\t', dtype='float')\n",
    "        length = len(data[:,])\n",
    "        if length > max_length:\n",
    "            length = max_length\n",
    "            data = data[:length]\n",
    "        \n",
    "        \n",
    "        # Comment/Uncomment to select data type\n",
    "        #plt.plot(np.linspace(1, length, length, endpoint=True), moving_average(data[:,1]), label=\"score \" + source[0], ls = source[2])\n",
    "        #plt.plot(np.linspace(1, length, length, endpoint=True), moving_average(data[:,2]), label=\"total_loss \" + source[0], ls = source[2])\n",
    "        #plt.plot(np.linspace(1, length, length, endpoint=True), moving_average(data[:,3]), label=\"actor_loss \" + source[0], ls = source[2])\n",
    "        #plt.plot(np.linspace(1, length, length, endpoint=True), moving_average(data[:,4]), label=\"critic_loss \" + source[0], ls = source[2])\n",
    "        #plt.plot(np.linspace(1, length, length, endpoint=True), moving_average(data[:,5]), label=\"entropy \" + source[0], ls = source[2])\n",
    "        plt.plot(np.linspace(1, length, length, endpoint=True), moving_average(abs(data[:,3]/data[:,4])), label=\"actor/critic \" + source[0], ls = source[2])\n",
    "    \n",
    "    plt.xlabel('Episode #')\n",
    "    plt.legend()\n",
    "    plt.xlim(0, length)\n",
    "    #plt.ylim(0, 1000)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "data_paths = []\n",
    "\n",
    "data_paths.append(['tennis','results/final/result.txt', \"-\"])\n",
    "plot(data_paths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
